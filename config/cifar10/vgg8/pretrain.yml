criterion:
  name: ce

optimizer:
  name: sgd
  lr: 0.0001
  weight_decay: 0.0003
  momentum: 0.9

scheduler:
  name: cosine
  lr_min: 0.0

run:
  experiment: "cifar10_vgg8_GEMM_pretrain"
  n_epochs: 20
  batch_size: 128
  use_cuda: 1
  gpu_id: 0
  deterministic: 1
  log_interval: 200
  train_noise: 0

quantize:
  quant_flag: True
  N_bits: 8
  N_bits_a: 8
  scaling_range_in: 1.
  scaling_range_out: 1.

noise:
  noise_flag: False
  noise_level: 0.005  #0.005 inference
  output_noise_level: 0.005 #0.005 inference
  weight_noise_std: 0.0 #For NA pre-training
  random_state: 42

checkpoint:
  save_best_model_k: 3
  checkpoint_dir: "cifar10/vgg/pretrain"
  model_comment: ""
  resume: 0
  restore_checkpoint: "checkpoint/cifar10/vgg/pretrain/SparseBP_GEMM_VGG8_N-8_Na-8__acc-88.00_epoch-192.pt"

model:
  name: "SparseBP_GEMM_VGG8"
  mode: "defender"
  block_list: [8, 8]
  act: relu
  act_thres: 6
  norm: bn


debug:
  verbose: 1

